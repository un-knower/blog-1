---
title: 数据质量管理
date: 2017-07-01 09:45:26
tags:
---

- Data Quality Management (DQS)

- 三要素
    + 数据梳理：摸清企业数据质量现状
        * 为数据质量提升提供一个全面的数据现状（类型、存储位置、业务使用、部门支撑、表现形态）参考
        * 理清数据分类及分类之间的关系，基础数据、组装的衍生数据；提升数据质量水平的突破点
        * 元数据的梳理占核心位置，是质量提升的前提和重点
    + 数据规范：高质量数据的保障
        * 数据标准：
        * 数据模型：

    + 数据生命周期：数据质量提升的切入点；数据和货币一样，流通起来的价值远远大于它静态的价值
        1. 计划
        2. 规范定义
        3. 开发上线
        4. 创建获取
        5. 维护使用
        6. 归档/恢复
        7. 清除

- 技术原则
    + 从需求开始控制数据质量
    + 在集成点检查数据质量
    + 持续积累检验规则
    + 自动化质量评分

- 实践
    + 基于系统建设的业务需求，分析数据标准规范，建立需求和标准的映射
    + 参考企业数据模型，依据本次业务需求设计出系统(项目)模型，系统模型的核心数据模型来源于企业数据模型
    + 建设的元数据管理，附加实现核查系统元数据是否符合企业数据模型规范，是否符合数据标准规范的工作
    + 对涉及数据集成整合的场景，要对数据生命周期中数据传输的几个环节进行数据质量监控和检核，也就是在集成点处进行监控
    + 若本次系统建设导致其他系统发生变更时，需要协同变更，是否发生变更时基于数据梳理实现元数据自动化管理所带来的直接保证

- 维
    1. data
    2. computer infrastructure
    3. storage infrastructure
    4. analysis
    5. visualization
    6. security&privacy

- 分类
    + 元数据
    + 主数据
    + 交易数据
    + 参考数据

- 统计指标


## 实践
### 一般类型字段

#### count
- in 1-10w：
- in 10w-1kw：
- in 1ww-100ww：
- in 1kww+ ：

### 三方数据更新频率
#### 更新变动判断依据
1. 条数
2. 数据唯一性，md5，SHA1 hash
- 定期查询数据条数

#### 某列数据是否有序，column  is order?
- 从而识别出增量的待选的 column 集
``` scala
def isOrder(): Unit = {
  val res = tableId + "\t\t" + columns.map(column => {
    val dt = df.schema.apply(column.getAttrName).dataType
    /**
      * 判断是否有序
      */
    val flag =
      try {
        if (dt.isInstanceOf[ByteType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.collect().map(_.getByte(0)).filter(_ != null).toList, count)
        } else if (dt.isInstanceOf[DecimalType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getDecimal(0)).filter(_ != null).collect().toList, count)
        } else if (dt.isInstanceOf[DoubleType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getDouble(0)).filter(_ != null).collect().toList, count)
        } else if (dt.isInstanceOf[FloatType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getFloat(0)).filter(_ != null).collect().toList, count)
        } else if (dt.isInstanceOf[IntegerType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getInt(0)).filter(_ != null).collect().toList, count)
        } else if (dt.isInstanceOf[LongType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getLong(0)).collect().filter(_ != null).toList, count)
        } else if (dt.isInstanceOf[ShortType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getShort(0)).collect().filter(_ != null).toList, count)
        } else if (dt.isInstanceOf[BooleanType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getBoolean(0)).collect().filter(_ != null).toList, count)
        } else if (dt.isInstanceOf[DateType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getDate(0)).filter(_ != null).map(_.getTime).collect().toList, count)
        } else if (dt.isInstanceOf[StringType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getString(0)).filter(_ != null).collect().toList, count)
        } else if (dt.isInstanceOf[TimestampType]) {
          IsOrderService.isOrder(df.select(column.getAttrName).rdd.map(_.getTimestamp(0)).filter(_ != null).map(_.getTime).collect().toList, count)
        } else {
          throw new Exception("类型未识别")
        }
      } catch {
        case e: Exception => {
          LOG.error(s"${column.getAttrName} 列数据处理程序异常", e)

          false
        }
      }
}

def isOrder[T](list: List[T], count: Long)(implicit ord: Ordering[T]): Boolean = {
  if (list == null || list.isEmpty | list.length < count * 0.9) return false

  list match {
    case Seq(h, t@_*) => list.zip(t).forall { case (x, y) => ord.gteq(x, y) } || list.zip(t).forall { case (x, y) => ord.lteq(x, y) }
    case _ => true
  }
}


```

#### missing count （缺失值）
- missing count 近似count 表明数据质量非常低
- missing count /count 越小越好，即表明数据质量高，更便于分析挖掘


#### unique count （唯一值）
- 识别外键foreign key
- threshold count 阈值常量，数据量级：1、10、100、1k、10k、100k、1000k、1kw
- count 近似 unique count 且近似 threshold count
    + if threshold count in 1、10、100
        * 怀疑为枚举，码表
            - in 1：性别码表、学历码表、
            - in 10：省份码表
            - in 100：城市码表、大学码表、上市公司码表
        * 直接存储所有unique distinct value，map[frequencyItems, count]
    + if threshold count in 100、1k、10k、100k、1000k、1kw
        * 怀疑为业务表
            - in 100：公司码表
            - in 1K：县区码表
            - in 10K：宾馆码表、小区码表、医院码表、超市码表
            - in 100K：
            - in 1000K：
            - in 1kw：
- count 数量级大于 unique count
    + 则该列使用了码表id作为外键
    + 如果 unique count类似，能否通过 相关性算法识别外键关联？

#### impl
``` scala
    /**
    * 构造获取唯一值及缺失值的sql查询语句
    *
    * @param tableName
    * @param columns
    * @return
    */
    def genDistinctStats(tableName: String, columns: String*): String = {
    val distinctBuffer = new StringBuffer()
    val missingBuffer = new StringBuffer()

    for (column <- columns) {
      distinctBuffer.append("count(distinct(" + column + ")) as unique_" + column + " , ")
      missingBuffer.append("sum(case when " + column + " is null then 1 else 0 end) as missing_" + column + ", ")
    }

    "select count(*) as count, " + distinctBuffer.toString + missingBuffer.deleteCharAt(missingBuffer.lastIndexOf(",")).toString.trim + " from " + tableName
    }

    val distinctMissingDf = spark.sql(genDistinctStats(table.getName, columns.map(_.getName): _*))
    val distinctMissingResult = distinctMissingDf.collect().map(row => {
      columns.map(column => (column.geTableId.toInt, column.getId.toInt, row.getAs[Long]("count"), row.getAs[Long]("missing_" + column.getName), row.getAs[Long]("unique_" + column.getName))).toArray
    }).reduce(_.++:(_))

    distinctMissingResult.foreach(info => println(info))
```


#### frequency items （频繁项 ）
- 提取topK 频繁项
- 随count数量级(order of magnitude)，而适当提取top(K* order)

##### impl
``` scala
    /**
      * 处理频繁字段
      */
    val freqDf = FrequentItems.singlePassFreqItems(df, columns.map(_.getAttrName).toSeq, 0.1)
    val freqResult = freqDf.collect().map(row => {
      columns.map(column =>
        (
          column.getId + " " + column.getTableId,
          row.getAs[scala.collection.Map[Any, Long]](column.getAttrName + "_freqItems").toArray.sortBy(_._2).mkString("\t") // 见下方代码改写记录
        )).toArray
    }).reduce(_.++:(_)).toMap[String, String]

    // 抽样统计实现
    val df = if (count > PropertyConstants.SAMPLE_COUNT) {
      LOG.info("数据量大于临界值，采取抽样技术进行数据质量统计")
      dfTmp.sample(false, PropertyConstants.SAMPLE_COUNT.toDouble / count.toDouble)
    } else dfTmp
```
##### 算法原理
paper：A Simple Algorithm for Finding Frequent Elements in Streams and Bags
``` scala
import org.apache.spark.sql._
import org.apache.spark.sql.types._

import scala.collection.mutable
import scala.collection.mutable.{Map => MutableMap}

/**
  * Created by likai on 2017/7/29.
  */
object FrequentItems {

  /** A helper class wrapping `MutableMap[Any, Long]` for simplicity. */
  class FreqItemCounter(size: Int) extends Serializable {
    val baseMap: MutableMap[Any, Long] = MutableMap.empty[Any, Long]

    /**
      * Add a new example to the counts if it exists, otherwise deduct the count
      * from existing items.
      */
    def add(key: Any, count: Long): this.type = {
      if (baseMap.contains(key)) {
        baseMap(key) += count
      } else {
        if (baseMap.size < size) {
          baseMap += key -> count
        } else {
          val minCount = if (baseMap.values.isEmpty) 0 else baseMap.values.min
          val remainder = count - minCount
          if (remainder >= 0) {
            baseMap += key -> count // something will get kicked out, so we can add this
            baseMap.retain((k, v) => v > minCount)
            baseMap.transform((k, v) => v - minCount)
          } else {
            baseMap.transform((k, v) => v - count)
          }
        }
      }
      this
    }

    /**
      * Merge two maps of counts.
      *
      * @param other The map containing the counts for that partition
      */
    def merge(other: FreqItemCounter): this.type = {
      other.baseMap.foreach { case (k, v) =>
        add(k, v)
      }
      this
    }
  }

  /**
    * Finding frequent items for columns, possibly with false positives. Using the
    * frequent element count algorithm described in
    * <a href="http://dx.doi.org/10.1145/762471.762473">here</a>, proposed by Karp, Schenker,
    * and Papadimitriou.
    * The `support` should be greater than 1e-4.
    * For Internal use only.
    *
    * @param df      The input DataFrame
    * @param cols    the names of the columns to search frequent items in
    * @param support The minimum frequency for an item to be considered `frequent`. Should be greater
    *                than 1e-4.
    * @return A Local DataFrame with the Array of frequent items for each column.
    */
  def singlePassFreqItems(
                           df: DataFrame,
                           cols: Seq[String],
                           support: Double): Seq[mutable.Map[Any, Long]] = {
    require(support >= 1e-4 && support <= 1.0, s"Support must be in [1e-4, 1], but got $support.")
    val numCols = cols.length
    // number of max items to keep counts for
    val sizeOfMap = (1 / support).toInt
    val countMaps = Seq.tabulate(numCols)(i => new FreqItemCounter(sizeOfMap))
    val originalSchema = df.schema
    val colInfo: Array[(String, DataType)] = cols.map { name =>
      val index = originalSchema.fieldIndex(name)
      (name, originalSchema.fields(index).dataType)
    }.toArray

    val freqItems = df.select(cols.map(new Column(_)): _*).rdd.aggregate(countMaps)(
      seqOp = (counts, row) => {
        var i = 0
        while (i < numCols) {
          val thisMap = counts(i)
          val key = row.get(i)
          thisMap.add(key, 1L)
          i += 1
        }
        counts
      },
      combOp = (baseCounts, counts) => {
        var i = 0
        while (i < numCols) {
          baseCounts(i).merge(counts(i))
          i += 1
        }
        baseCounts
      }
    )

    freqItems.map(_.baseMap)
  }
}
```


### 数值类型
#### min 最小值

#### max 最大值

#### mean 平均值

#### stddev 标准差

#### 实现
``` scala
    val statsDf = df.describe(columns.map(_.getName): _*)
    val result = statsDf.collect().map(row => {
      columns.map(column => (column.getId, row.getAs[String](column.getName))).toMap
    })

    val countBuffer = result.apply(0)
    val meanBuffer = result.apply(1)
    val stddevBuffer = result.apply(2)
    val minBuffer = result.apply(3)
    val maxBuffer = result.apply(4)
```

#### median 中位数
``` scala
    /**
      * 处理中位数
      */
    val medianMap = columns.map(_.getName).map(col => {
      try {
        df.stat.approxQuantile(col, Array(0.5), 0.25).map(num => (col, num))
      } catch {
        case nsee: java.util.NoSuchElementException => {
          Array((col, 0.0))
        }
      }
    }).reduce(_.++:(_)).map(info => (nameIdMap.getOrElse(info._1, null).toInt, info._2)).toMap
```







+ 代码改写记录：
- org.apache.spark.sql.execution.stat.FrequentItems -> org.apache.spark.sql.execution.stat.ChaosFrequentItems
``` scala
    //    val justItems = freqItems.map(m => m.baseMap.keys.toArray)
    val resultRow = Row(freqItems.map(_.baseMap): _*) // 原先只返回item，改为既返回item又返回对应count
    // append frequent Items to the column name for easy debugging
    val outputCols = colInfo.map { v =>
    //    StructField(v._1 + "_freqItems", ArrayType(v._2, false))
      StructField(v._1 + "_freqItems", MapType(v._2, LongType, false)) // 重新构造对应schema
    }
    val schema = StructType(outputCols).toAttributes
    Dataset.ofRows(df.sparkSession, LocalRelation.fromExternalRows(schema, Seq(resultRow)))
```

- org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils
``` scala
    private def getCatalystType(sqlType: Int, precision: Int, scale: Int, signed: Boolean): DataType = {
        // ...
    /**
      * 更改，解决getLong异常
      */
    case java.sql.Types.ROWID => StringType
    //      case java.sql.Types.ROWID         => LongType
    }
        // ...
```



#### 数据存储结构

``` sql
CREATE TABLE `stats_general_attr` (
  `stats_general_attr_id` int(11) NOT NULL AUTO_INCREMENT,
  `table_type` tinyint(4) DEFAULT NULL,
  `table_id` int(11) DEFAULT NULL,
  `attr_id` int(11) DEFAULT NULL,
  `count` bigint(20) DEFAULT NULL,
  `missing_count` bigint(20) DEFAULT NULL,
  `unique_count` bigint(20) DEFAULT NULL,
  `freq_items` mediumtext COLLATE utf8_unicode_ci,
  `is_order` tinyint(4) DEFAULT NULL,
  `is_sample` tinyint(4) DEFAULT NULL,
  `describe` varchar(45) COLLATE utf8_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`stats_general_attr_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci COMMENT='一般字段数据质量统计';

CREATE TABLE `stats_num_attr` (
  `stats_num_attr_id` int(11) NOT NULL AUTO_INCREMENT,
  `stats_general_attr_id` int(11) DEFAULT NULL,
  `count` bigint(20) DEFAULT NULL,
  `mean` double DEFAULT NULL,
  `stddev` double DEFAULT NULL,
  `min` double DEFAULT NULL,
  `max` double DEFAULT NULL,
  `median` double DEFAULT NULL,
  `mode` double DEFAULT NULL,
  `is_sample` tinyint(4) DEFAULT NULL,
  `describe` varchar(45) COLLATE utf8_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`stats_num_attr_id`),
  KEY `stats_general_attr_id_idx` (`stats_general_attr_id`),
  CONSTRAINT `stats_general_attr_id` FOREIGN KEY (`stats_general_attr_id`) REFERENCES `stats_general_attr` (`stats_general_attr_id`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci COMMENT='数值类型属性数据质量统计记录表';

```

## 问题记录
1. INTERVAL 关键字

``` shell
org.apache.spark.sql.catalyst.parser.ParseException: 
at least one time unit should be given for interval literal(line 1, pos 0)

== SQL ==
INTERVAL
^^^

  at org.apache.spark.sql.catalyst.parser.ParserUtils$.validate(ParserUtils.scala:80)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitInterval$1.apply(AstBuilder.scala:1377)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitInterval$1.apply(AstBuilder.scala:1375)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:1375)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:45)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalContext.accept(SqlBaseParser.java:12180)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:59)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitIntervalLiteral(SqlBaseBaseVisitor.java:1007)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalLiteralContext.accept(SqlBaseParser.java:11774)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:59)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitConstantDefault(SqlBaseBaseVisitor.java:958)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ConstantDefaultContext.accept(SqlBaseParser.java:11117)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:59)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitValueExpressionDefault(SqlBaseBaseVisitor.java:881)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:10814)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:49)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:760)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitPredicated$1.apply(AstBuilder.scala:896)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitPredicated$1.apply(AstBuilder.scala:895)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:895)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:45)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:10560)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:59)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitBooleanDefault(SqlBaseBaseVisitor.java:853)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$BooleanDefaultContext.accept(SqlBaseParser.java:10354)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:59)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitExpression(SqlBaseBaseVisitor.java:832)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:10302)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:49)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:760)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitNamedExpression$1.apply(AstBuilder.scala:782)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitNamedExpression$1.apply(AstBuilder.scala:781)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:781)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleExpression$1.apply(AstBuilder.scala:70)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleExpression$1.apply(AstBuilder.scala:70)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleExpression(AstBuilder.scala:69)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parseExpression$1.apply(ParseDriver.scala:44)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parseExpression$1.apply(ParseDriver.scala:43)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:82)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:43)
  at org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1157)
  at org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1156)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1156)
  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:206)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:404)
  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:358)
```
- 诊断：表属性字段带有关键字 INTERVAL
